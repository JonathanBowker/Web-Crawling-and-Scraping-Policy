# Web Scraping Policy
## Advanced Analytica's policy for web crawling

## Background
"Crawler" (sometimes also called a "robot" or "spider") is a generic term for any program that is used to automatically discover and scan websites by following links from one web page to another. Advanced Analytica's crawler is called - Securicrawl®. 

Use of alternative data sources is vital for Advanced Analytica to achieve its objective of monitoring regulations, this includes risks caused by the way in which clients interest in relation to regulations). Web Crawling is one of these alternative data sources often used to supplement existing data sources. Therefore, the purpose of this policy is to ensure Web Crawling activities are conducted transparently, ethically and with respect to all relevant legislation.

This policy sets out the practices and procedures that Advanced Analytica will follow when carrying out web scraping. Web scraping is defined as the automatic collection of data retrieved from the internet, typically from HTML or PDF, using a piece of software or program.

## Scope
This policy sets out the practices and procedures that Advanced Analytica employees, temporary and casual staff, and any contractor, researcher or academic partner carrying out work for the agency or acting on our behalf must follow when carrying out web scraping.

When assessing external organisations who may scrape on behalf of Advanced Analytica, either via business as usual activities or on an ad-hoc basis, this policy will be used to guide assurance activities. This is to ensure appropriate governance structures are in place when gathering and processing data gathered via scraping activities.

**Note that this document does not cover the use of Application Programming Interfaces (API’s) and applies only to Advanced Analytica.**

## Next Review
We aim to keep all guidance up to date and undertake regular reviews to ensure this guidance remains relevant. The next scheduled review date for this guidance is 01/06/2023

## Compliance
All Advanced Analytica staff must comply with this policy in any project involving scraping data from the web. Failure to comply may result in disciplinary action in line with Advanced Analytica’s discipline policy.

## Policy Statement
Advanced Analytica is carrying out web-scraping activity for the purposes of its business objectives functions in particular for selling insights derived from the analysis of data collected from alternative data sources.

Advanced Analytica will adopt the following principles to guide our web scraping activities:

- Seek to minimise the burden on scraped sites
- Abide by all applicable legislation and monitor the evolving legal and ethical situation
- Protect all personal data


## Policy Details

### Seek to minimise the burden on scraped sites
Excessive use of web scraping can be burdensome, and overuse on a single webpage may have a detrimental impact to the operation of the website. To avoid such scenarios, we will maintain a register of web scraping activity and follow these practices, where applicable, and always act within the law:

- Assess other data collection methods, such as RSS/Atom feeds or API’s, before considering the use of web scraping
- Respect scraping limitations enforced via documents like the Robots Exclusion Protocol (Robots.txt) and Site Terms and Conditions
- Delay accessing pages on the same domain
- Adding idle time between requests
- Limiting the depth of crawl within the given domain
- When scraping multiple domains, parallelising the crawling operation to minimise successive requests to the same domain
- Scraping at a time of day when the website is unlikely to be experiencing heavy traffic - for example, early in the morning or late in the night
- Optimising the web scraping strategy to minimise volumes of requests to domains
- Only collect the parts of pages required for the initial purpose
- Include a User Agent String to differentiate Advanced Analytica from other users

Please note that this list of best practices is non-exhaustive and Advanced Analytica will strive to maintain a high-standard of web-scraping by monitoring and implementing best practices.

## Abide by applicable legislation and monitor the evolving legal and ethical situation

Advanced Analytica understands that the legal situation surrounding web scraping is complex and ever evolving; and there are relatively few relevant legal precedents for conducting such activities. Advanced Analytica will continue to monitor the legal situation as it evolves and amend our approach accordingly. Likewise, Advanced Analytica recognises the use of web scraping and the processing of data collected in this manner may give rise to a variety of ethical issues and challenges. Efforts are currently underway to develop a practical framework for considering and mitigating potential ethical risks both directly and indirectly attributed to web scraping.

Those who wish to undertake web scraping activities must document and justify the rationale, the legal and ethical reasoning and the benefits of conducting the web scraping. This rationale will be used as the basis for approval from the Information Governance Board and will be scrutinised during the assessment phase to ensure continued alignment with applicable legislation. See section 9 for more details on this.

## Protect all personal data
Advanced Analytica is fully commited to compliance with the [Data Protection Act 2018 and the General Data Protection Regulation](https://www.legislation.gov.uk/eur/2016/679/contents)) (Opens in a new window) and ensuring that personal data is used lawfully, fairly and transparently. Data protection is of great importance to the Advanced Analytica, not only because it is critical to the work of our business, but also because it ensures we protect individual privacy and maintain public confidence in the company.

# Web Crawling Policy
## Advanced Analytica's policy for Web Crawling

## Background
Use of alternative data sources is vital for Advanced Analytica to achieve its business objective of monitoring UK legislation and regulations to identify risks in relation to changing regulations. Web Scraping is one of these alternative data sources often used to supplement existing data sources. Therefore, the purpose of this policy is to ensure Web Scraping activities are conducted transparently, ethically and responsibly with respect to all relevant legislation.

This policy sets out the practices and procedures that Advanced Analytica will follow when carrying out Web Scraping. 

Web Crawling is defined as an internet bot or Web Crawler that systematically browses the World Wide Web for the purposes of link indexing for information gathering. Web scraping is defined as the automatic collection of data from the internet using links provided by a Web Crawler, typically from HTML or PDF or CSV files, using a piece of software or program called a Web Scraper. Advanced Analytica's combined Web Crawling and Web Scraping System is called - SecuriCrawl®.

## Scope
This policy sets out the practices and procedures that Advanced Analytica follows when carrying out Web Crawling and Web Scraping (SecuriCrawl®) activities with Securicrawl® on the behalf its clients. 

**Note that this document does not cover the use of Application Programming Interfaces (API’s) and applies only to Advanced Analytica.**

## Next Review
We aim to keep all guidance up to date and undertake regular reviews to ensure this guidance remains relevant. The next scheduled review date for this guidance is 01/06/2023

## Compliance
All Advanced Analytica staff must comply with this policy in any project involving data collection from the web. Failure to comply may result in disciplinary action in line with Advanced Analytica’s discipline policy.

## Policy Statement
Advanced Analytica is carrying out SecuriCrawl® activity for the purposes of its business objectives in particular for the detection in changes to legislation and regulations and selling insights derived from the analysis of data collected from alternative data sources.

Advanced Analytica will adopt the following principles to guide SecuriCrawl® activities:

- Seek to minimise the burden on scraped sites
- Abide by all applicable legislation and monitor the evolving legal and ethical situation
- Protect all personal data


## Policy Details

### Seek to minimise the burden on scraped sites
Excessive use of Web Scraping can be burdensome, and overuse on a single webpage may have a detrimental impact to the operation of the website. To avoid such scenarios, we will maintain a register of SecuriCrawl® activity and follow these practices, where applicable, and always act within the law:

- Assess other data collection methods, such API’s or Webhooks, before considering the use of SecuriCrawl®
- Respect scraping limitations enforced via documents like the Robots Exclusion Protocol (Robots.txt) and Site Terms and Conditions
- Delay accessing pages on the same domain
- Adding idle time between requests
- Limiting the depth of crawl for link collection within the given domain
- When scraping multiple domains, parallelising the crawling operation to minimise successive requests to the same domain
- Scraping at a time of day when the website is unlikely to be experiencing heavy traffic - for example, early in the morning or late in the night
- Optimising the scraping strategy to minimise volumes of requests to domains
- Only collect the parts of pages required for the initial purpose
- Include a User Agent String to differentiate SecuriCrawl® from other users with a link to this policy

Please note that this list of best practices is non-exhaustive and Advanced Analytica will strive to maintain a high-standard for SecuriCrawl® operations by monitoring and implementing best practices for AI data protection guidelines.

## Abide by applicable legislation and monitor the evolving legal and ethical situation


Advanced Analytica understands that the legal situation surrounding Web Crawling and Web Scraping is complex and ever evolving; and there are relatively few relevant legal precedents for conducting such activities. Advanced Analytica will continue to monitor the legal situation as it evolves and amend our approach accordingly. Likewise, Advanced Analytica recognises the use of SecuriCrawl® and the processing of data collected in this manner may give rise to a variety of ethical issues and challenges. To address these issues Advanced Analytica has implemented the [Guidance on AI and Data Protectionin](https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-artificial-intelligence-and-data-protection/) and the [AI and Data Protection Risk Toolkit](https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-ai-and-data-protection/ai-and-data-protection-risk-toolkit/) to mitigate potential ethical risks both directly and indirectly attributed to SecuriCrawl® operations.

Clients who wish to engage the SecuriCrawl® service must document and justify the rationale, the legal and ethical reasoning and the benefits of conducting the programme. This rationale will be used as the basis for approval by Advanced Analytica and will be scrutinised during the assessment phase to ensure continued alignment with applicable legislation. 

## Personal data protection and artificial intelligence (AI)
Advanced Analytica is fully commited to compliance with the [Data Protection Act 2018 and the General Data Protection Regulation](https://www.legislation.gov.uk/eur/2016/679/contents)) (Opens in a new window) and ensuring that personal data is used lawfully, and fairly and transparently protected in particlar where artificial intelligence is used. Data protection is of great importance to Advanced Analytica, not only because it is critical to the practice of our business, but also because it ensures we protect individual privacy and maintain public confidence in our business.

# Accompanying guidance
## Web Crawling steps
### Web Crawling request

 - Advanced Analytica clients wishing to engage SecuriCrawl® services should contact Advanced Analytica as soon as possible after identifying the requirement
 - A SecuriCrawl® request document [LINK] must be completed and attached to the correspondence for review
 - Please note that the Advanced Analytica may conduct a feasibility crawl on an authorised target website. During this crawl no data will be stored by Advanced Analytica. Advanced Analytica will ensure that this process remains in line with policy.

### SecuriCrawl® Evaluation
 - Clients will provide Advanced Analytica with a new request and completed request document
 - Advanced Analytica will evaluate the request document and verify whether SecuriCrawl® can be lawfully engaged 
 - Advanced Analytica will perform relevant AI risk assessments in line with the [ICO Guidance on AI and data protection](https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-artificial-intelligence-and-data-protection/)

### Web Crawling Log
Advanced Analytica will update the central crawling log with the approved activities and inform the business unit they can begin scraping. This is to ensure that the correct level of due dilligence is assigned to each project and that all activities are recorded for auditability and accountability purposes in the Web Crawling Log.

### SecuriCrawl® Operations
Web Crawling using activities using SecuriCrawl® can commence and will only be undertaken in accordance with this Web Crawling Policy and corresponding best practices.

# Glossary
### Application Programming Interface (API)
In terms of web scraping, an API can be built by the owner of a website to allow easy access to data held on the site without the need for building a web scraper.

### Depth of Crawl
Depth of crawl refers to how far into a websites page hiearchy a web-crawler accesses. A websites homepage is the top of this hierarchy (level 0), pages linked from the home page are at level 1, pages linked from level 1 pages are level 2 and so forth. Limiting the depth of crawl therefore means limiting which level the web-crawler will access.

### Hypertext Markup Language
The standard markup language for defining the structure of web pages. HTML elements are used to tell the browser how to display the contents of a web page.

### Idle Time
Idle time, or sleep time, is the pause between each request made to a website by the scraper.

### Parallelising the Crawling
A parallel crawler is a crawler that runs multiple processes in parallel with the goal to maximise download rates and minimise scraping overheads. For example, if website A and B contain multiple webages, then a parallelised crawl might capture a page from website A, followed by a page from website B and so on. If crawls are not parallelised, then the overhead on a single website is likely to be vastly increased.

### Robots Exclusion Protocol
Also known as the Robots.txt file. This is a standard used by websites to communicate with web crawlers and other web robots. Essentially it provides website owners a method to restrict part of their websites from scraping or limit scraping entirely to just certified scrapers such as search engines. More information on this subject can be found on the robots.txt website (Opens in a new window).

### User Agent String
A User Agent String is a custom piece of text that can be added to a scraper allowing the website owner to identify the operator and/or the purpose of the web scraping activity being undertaken. This can be modified when creating the web scraper and is often used for transparencies sake.

### Web Crawler
A web crawler, sometimes referred to as a spider, is an internet bot that systematically browses the World Wide Web for the purposes of information gathering. The most common example of this is search engines like Google who use web crawlers to keep their search results up-to-date.

### Web Scraping
Web scraping is defined as the automatic collection of data retrieved from the internet using a piece of software or program.



